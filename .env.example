# Research Agent Environment Variables
# Copy this file to .env and fill in your values
# NEVER commit .env to version control

# =============================================================================
# API KEYS (Required)
# =============================================================================

# OpenAI API key for GPT-4o-mini workers
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-openai-api-key-here

# Tavily API key for web search
# Get from: https://app.tavily.com/
# Free tier: 1,000 searches/month
TAVILY_API_KEY=tvly-your-tavily-api-key-here

# =============================================================================
# BRAIN SERVICE (Self-hosted vLLM)
# =============================================================================

# URL of the brain service (vLLM server)
# For local development: http://localhost:8001
# For Brev.dev: http://<brev-instance>:8001
BRAIN_SERVICE_URL=http://localhost:8001

# API key for brain service (internal auth)
# Generate a random string for this
BRAIN_API_KEY=your-internal-brain-api-key

# Model to use for brain service
BRAIN_MODEL_NAME=deepseek-ai/DeepSeek-R1-Distill-Qwen-14B

# =============================================================================
# APPLICATION SETTINGS
# =============================================================================

# Environment: development, staging, production
ENVIRONMENT=development

# API server settings
API_HOST=0.0.0.0
API_PORT=8000

# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Enable JSON logging (true for production)
LOG_JSON=false

# =============================================================================
# RATE LIMITING
# =============================================================================

# Requests per minute per IP
RATE_LIMIT_REQUESTS=60
RATE_LIMIT_WINDOW_SECONDS=60

# =============================================================================
# OUTPUT SETTINGS
# =============================================================================

# Directory for generated papers (relative to project root)
OUTPUT_DIR=outputs

# Compile LaTeX to PDF (requires pdflatex installed)
COMPILE_PDF=true

# =============================================================================
# WORKER SETTINGS
# =============================================================================

# Maximum parallel search workers
MAX_SEARCH_WORKERS=5

# Timeout for external API calls (seconds)
API_TIMEOUT_SECONDS=30

# Maximum retries for failed API calls
MAX_RETRIES=3

# =============================================================================
# RESEARCH WORKFLOW SETTINGS
# =============================================================================

# Maximum search iterations before stopping
MAX_SEARCH_ITERATIONS=3

# Maximum tokens for brain context
MAX_CONTEXT_TOKENS=128000

# Temperature for brain model (0.5-0.7 recommended for DeepSeek-R1)
BRAIN_TEMPERATURE=0.6

# Top-p for brain model
BRAIN_TOP_P=0.95

# =============================================================================
# DATABASE (PostgreSQL)
# =============================================================================

# PostgreSQL connection settings
POSTGRES_USER=research
POSTGRES_PASSWORD=changeme
POSTGRES_DB=research_agent

# Full database URL (constructed from above, or override directly)
# DATABASE_URL=postgresql://research:changeme@localhost:5432/research_agent

# =============================================================================
# REDIS (Caching and Task Queue)
# =============================================================================

# Redis password
REDIS_PASSWORD=changeme

# Full Redis URL (constructed from above, or override directly)
# REDIS_URL=redis://:changeme@localhost:6379/0

# =============================================================================
# OPTIONAL: OBSERVABILITY (Production)
# =============================================================================

# OpenTelemetry endpoint (if using tracing)
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317

# Prometheus metrics port
# METRICS_PORT=9090
